{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just to make sure you have installed all the packages that you will need in Lab-3\n",
    "for p in (\"Knet\",\"ArgParse\",\"Compat\",\"GZip\")\n",
    "    Pkg.installed(p) == nothing && Pkg.add(p)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Knet   \n",
    "using ArgParse # To work with command line argumands\n",
    "using Compat,GZip # Helpers to read the MNIST (Like lab-2)\n",
    "using Knet: relu_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main (generic function with 2 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function main(args=\"\")\n",
    "    #=\n",
    "    In the macro, options and positional arguments are specified within a begin...end block\n",
    "    by one or more names in a line, optionally followed by a list of settings. \n",
    "    So, in  the below, there are five options: epoch,batchsize,hidden size of mlp, \n",
    "    learning rate, weight initialization constant\n",
    "    =#\n",
    "    s = ArgParseSettings()\n",
    "    @add_arg_table s begin\n",
    "        (\"--epochs\"; arg_type=Int; default=10; help=\"number of epoch \")\n",
    "        (\"--batchsize\"; arg_type=Int; default=100; help=\"size of minibatches\")\n",
    "        (\"--hidden\"; nargs='*'; arg_type=Int; help=\"sizes of hidden layers, e.g. --hidden 128 64 for a net with two hidden layers\")\n",
    "        (\"--lr\"; arg_type=Float64; default=0.5; help=\"learning rate\")\n",
    "        (\"--winit\"; arg_type=Float64; default=0.1; help=\"w initialized with winit*randn()\")\n",
    "    end\n",
    "\n",
    "    #=\n",
    "    the actual argument parsing is performed via the parse_args function the result \n",
    "    will be a Dict{String,Any} object.In our case, it will contain the keys \"epochs\", \n",
    "    \"batchsize\", \"hidden\" and \"lr\", \"winit\" so that e.g. o[\"lr\"] or o[:lr] \n",
    "     will yield the value associated with the positional argument.\n",
    "     For more information: http://argparsejl.readthedocs.io/en/latest/argparse.html\n",
    "    =#\n",
    "    o = parse_args(s; as_symbols=true)\n",
    "\n",
    "    # Some global configs do not change here \n",
    "    println(\"opts=\",[(k,v) for (k,v) in o]...)\n",
    "    o[:seed] = 123\n",
    "    srand(o[:seed])\n",
    "\n",
    "    # initalize weights of your model \n",
    "    w = weights(o[:hidden]; winit=o[:winit])\n",
    "\n",
    "    # load the mnist data\n",
    "    xtrnraw, ytrnraw, xtstraw, ytstraw = loaddata()\n",
    "    \n",
    "    xtrn = convert(Array{Float32}, reshape(xtrnraw ./ 255, 28*28, div(length(xtrnraw), 784)))\n",
    "    ytrnraw[ytrnraw.==0]=10;\n",
    "    ytrn = convert(Array{Float32}, sparse(convert(Vector{Int},ytrnraw),1:length(ytrnraw),one(eltype(ytrnraw)),10,length(ytrnraw)))\n",
    "    \n",
    "    xtst = convert(Array{Float32}, reshape(xtstraw ./ 255, 28*28, div(length(xtstraw), 784)))\n",
    "    ytstraw[ytstraw.==0]=10;\n",
    "    ytst = convert(Array{Float32}, sparse(convert(Vector{Int},ytstraw),1:length(ytstraw),one(eltype(ytstraw)),10,length(ytstraw)))\n",
    "    # seperate it into batches. \n",
    "    dtrn = minibatch(xtrn, ytrn, o[:batchsize]) \n",
    "    dtst = minibatch(xtst, ytst, o[:batchsize])\n",
    "\n",
    "    # helper function to see how your training goes on.\n",
    "    report(epoch)=println((:epoch,epoch,:trn,accuracy(w,dtrn),:tst,accuracy(w,dtst)))\n",
    "    \n",
    "    report(0)    \n",
    "    # Main part of our training process \n",
    "    @time for epoch=1:o[:epochs]\n",
    "        train(w, dtrn,o[:lr])\n",
    "        report(epoch)\n",
    "    end\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Weights function\n",
    "If we have the first hidden layer H1(h1x28) h1 - number of hidden nodes and second hidden layer H2(h2xh1)\n",
    "So we would first create a random weights matrix with y,x and with y,1 for the output layer node\n",
    "randn - Generate a normally-distributed random number with mean 0 and standard deviation 1. Optionally generate an array of normally-distributed random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weights (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create weight matrix that will be used in our MLP.\n",
    "# your weights should be  Float32 type.\n",
    "function weights(h;winit=0.1)\n",
    "    w = Any[]    \n",
    "    x = 28*28\n",
    "    for y in [h..., 10]\n",
    "        push!(w, convert(Array{Float32}, winit*randn(y,x)))\n",
    "        push!(w, convert(Array{Float32}, zeros(y, 1)))\n",
    "        x = y\n",
    "    end\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Minibatch function \n",
    "\n",
    "Starting with index 1 and proceed in steps of batch size till you get to the last possible batch start index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minibatch (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the minibatches on Mnist data. You will do exactly the same thing you did in Lab-2\n",
    "#takes raw input (X) and gold labels (Y)\n",
    "#returns list of minibatches (x, y)\n",
    "function minibatch(X, Y,bs)\n",
    "    #takes raw input (X) and gold labels (Y)\n",
    "    #returns list of minibatches (x, y)\n",
    "    cols_X = size(X,1)\n",
    "    rows_X = size(X,2)\n",
    "    data = Any[]\n",
    "    # MY CODE HERE\n",
    "    for batch_start=1:bs:rows_X-bs+1\n",
    "        batch_end=batch_start+bs-1\n",
    "        push!(data,(X[:,batch_start:batch_end],Y[:,batch_start:batch_end]))\n",
    "    end\n",
    "    #MY CODE ENDS HERE\n",
    "    return data\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Predict Function\n",
    "\n",
    "We iteratively update the x value by using the logic w*input_x .+ w+1 and we are using the relu_dot function which is basically a direct way to write ReLu without using the max function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=\n",
    "predict function takes model parameters (w) and data (x) and \n",
    "makes forward calculation. Fill below function accordingly. It\n",
    "should return #ofclasses x batchsize size matrix as a result. \n",
    "Use ReLU nonlinearty at each layer except the last one. \n",
    "=#\n",
    "function predict(w,x)\n",
    "    for i=1:2:length(w)\n",
    "        x = w[i]*x .+ w[i+1]\n",
    "        if i<length(w)-1\n",
    "            x = relu_dot(x)\n",
    "        end\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 & 1.5 - Loss function and gradient of the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::gradfun) (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=\n",
    "loss function takes model parameters(w), a batch of instance (x) and\n",
    "their gold labels. Please fill the loss function so that it returns \n",
    "the loss based on your predictions and gold data\n",
    "Hint : You may want to use predict function here to get your predictions \n",
    "Hint2: Take a loot at logp function defined in knet\n",
    "=#\n",
    "function loss(w,x,ygold)\n",
    "    ypred = predict(w,x)\n",
    "    #logp returns normalized log probabilities\n",
    "    ylogp_norm = logp(ypred,1)\n",
    "    -sum(ygold .* ylogp_norm) / size(ygold,2) #sum(abs2,ypred-ygold)\n",
    "end\n",
    "\n",
    "#=\n",
    "  As you notice, we did not do anything to calculate gradients of our loss function.\n",
    "  We know that Knet will do it on behalf of us. However, we need to provide our loss function\n",
    "  to Knet so that it can record all necessary operations. The line below does this.\n",
    "=#\n",
    "#Just doing a gradient on this is enough\n",
    "lossgradient = grad(loss)  # your code here [just 1 line]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6 - Training function \n",
    "First to perform backpropagation of the weights - we calculate the graident of loss with the present w and input outputs . And then we update all the weights based on the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=\n",
    "This is the main function you need to use to train your model.\n",
    "It takes model of parameter, all training data, learning rate and \n",
    "epoch which determines the number of times you will go through your\n",
    "whole data. Inside this function, you need to decide your prediction\n",
    ",measure the loss between  gold labels and your predictions. And finally\n",
    "update your weights with the gradients Knet gives  you based on the loss.\n",
    "Hint: You did everything required in this function (by implementing loss and predict functions) except the last step. Here you just need to call higher order procedure given you by Knet and update the weights accordingly.\n",
    "=#\n",
    "function train(w, dtrn,lr)\n",
    "    for (x,y) in dtrn\n",
    "        g = lossgradient(w, x, y)\n",
    "        for i in 1:length(w)\n",
    "            w[i] = (-lr)*g[i]+w[i]\n",
    "        end\n",
    "    end\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7 Accuracy Measurement - uses principles from the loss function\n",
    "\n",
    "For the given test data \n",
    "\n",
    "1. Find what the output of the model is for each row\n",
    "2. Normalize the outputs by using the logp function again like in the caluclation of loss\n",
    "3. Calculate the loss for this row and add to the total loss\n",
    "4. Check how many labels were predicted right\n",
    "5. Return share of wrongly predicted labels and correctly predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=\n",
    "Function for measuring current accuracy of the model.  Again you need to find your predictions first and then measure how many of them correct. You also need to measure average loss your model does at test dataset after each epoch.\n",
    "=#\n",
    "using Compat\n",
    "import Compat.String\n",
    "\n",
    "function accuracy(w,dtst,pred=predict)\n",
    "    total_rows = 0\n",
    "    got_right = 0\n",
    "    got_wrong = 0\n",
    "    for (x, ygold) in dtst\n",
    "        ypred = pred(w, x)\n",
    "        ynorm = logp(ypred,1) \n",
    "        got_wrong += -sum(ygold .* ynorm)\n",
    "        got_right += sum(ygold .* (ypred .== maximum(ypred,1)))\n",
    "        total_rows += size(ygold,2)\n",
    "    end\n",
    "    return (got_right/total_rows, got_wrong/total_rows)\n",
    "end\n",
    "function loaddata()\n",
    "\tinfo(\"Loading MNIST...\")\n",
    "\txtrn = gzload(\"train-images-idx3-ubyte.gz\")[17:end]\n",
    "\txtst = gzload(\"t10k-images-idx3-ubyte.gz\")[17:end]\n",
    "\tytrn = gzload(\"train-labels-idx1-ubyte.gz\")[9:end]\n",
    "\tytst = gzload(\"t10k-labels-idx1-ubyte.gz\")[9:end]\n",
    "\treturn (xtrn, ytrn, xtst, ytst)\n",
    "end\n",
    "\n",
    "function gzload(file; path=\"$file\", url=\"http://yann.lecun.com/exdb/mnist/$file\")\n",
    "\tisfile(path) || download(url, path)\n",
    "\tf = gzopen(path)\n",
    "\ta = @compat read(f)\n",
    "\tclose(f)\n",
    "\treturn(a)\n",
    "end\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementation details\n",
    "\n",
    "# For the default configuration \n",
    "\n",
    "# min epoch correctly predicted labels accuracy: 0.9049f0\n",
    "# max epoch correctly predicted labels accuracy: 0.9151f0\n",
    "# min epoch wrongly predicted labels accuracy: 0.29300820530955446\n",
    "# max epoch wrongly predicted labels accuracy: 0.3238402942176069\n",
    "\n",
    "\n",
    "# With hidden size --32 \n",
    "\n",
    "# min epoch correctly predicted labels accuracy: 0.9282f0\n",
    "# max epoch correctly predicted labels accuracy: 0.9595f0\n",
    "# min epoch wrongly predicted labels accuracy: 0.12928239247059078\n",
    "# max epoch wrongly predicted labels accuracy: 0.21917876140175835\n",
    "\n",
    "\n",
    "# With hidden size --64\n",
    "\n",
    "# min epoch correctly predicted labels accuracy: 0.9383f0\n",
    "# max epoch correctly predicted labels accuracy: 0.9682f0\n",
    "# min epoch wrongly predicted labels accuracy: 0.10537905940245612\n",
    "# max epoch wrongly predicted labels accuracy: 0.19680877156377527\n",
    "\n",
    "\n",
    "# With hidden size --64 64 64 64\n",
    "\n",
    "# min epoch correctly predicted labels accuracy: 0.1161f0\n",
    "# max epoch correctly predicted labels accuracy: 0.968f0\n",
    "# min epoch wrongly predicted labels accuracy: 0.18178857633542703\n",
    "# max epoch wrongly predicted labels accuracy: 2.3000147f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2.1 Hidden layers vs Performance\n",
    "\n",
    "# We can observe that increasing the number of hidden layers doesn't increase the accuracy\n",
    "# In fact the perfomance can worsen as we can see from the results our training. In some epochs the accuracy \n",
    "# has fallen to min 0.116 for correctly predicted labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2.2 Configurations vs Overfitting\n",
    "\n",
    "# Overfitting is when the model fits the training data so well that it performs badly on the test/new\n",
    "# dataset. I looked at the implementation results to find the instance in which there is a decrease \n",
    "# in accuracy between training and testing.And for the configuration 64 64 64 64 hidden size, there \n",
    "# is a significantly bigger difference in accuracy during training and testing compared to rest\n",
    "# of the configurations. Having way too many hidden layers could result in overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2.3 Learning rate set to 4.0\n",
    "\n",
    "# I noticed that setting learning rate to 4.0 has improved my training time to 47 seconds while rest\n",
    "# of the configurations took around 200s or 150s . But the training rate has dropped significantly\n",
    "# to around 89% compared to 95+% or above in the rest of the configurations\n",
    "\n",
    "# Intuition for why this might be the case : Seems like learning rate during the training step\n",
    "# is intended as a way to determine how fast the model should react to a weird data outlier\n",
    "# in the training dataset.The higher the learning rate the faster (the more erratic) the model \n",
    "# is during training. Which is a good thing when we want the model to be sensitive to data changes\n",
    "# I think we want a good balance between a low learning rate and a high learning rate\n",
    "# so that there is a good balance between the model finishing training to a good accuracy and \n",
    "# also being able to be responsive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
